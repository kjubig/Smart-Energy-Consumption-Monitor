# docker-compose.yml (fragmenty)

services:
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092   # ← ważne: host 'kafka'
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    ports: ["9092:9092"]
    networks: [ smarthub ]
    healthcheck:
      test: ["CMD", "bash", "-lc", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server kafka:9092 --list >/dev/null 2>&1 || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
      start_period: 20s
    restart: unless-stopped

  spark-master:
    image: apache/spark:3.5.1
    container_name: spark-master
    command: [ "/bin/bash", "-lc", "/opt/spark/bin/spark-class org.apache.spark.deploy.master.Master" ]
    environment: [ "SPARK_NO_DAEMONIZE=true" ]
    ports: ["7077:7077", "8080:8080"]
    networks: [ smarthub ]
    restart: unless-stopped

  spark-worker:
    image: apache/spark:3.5.1
    container_name: spark-worker
    command: [ "/bin/bash", "-lc", "/opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077" ]
    environment: [ "SPARK_NO_DAEMONIZE=true" ]
    depends_on: [ spark-master ]
    networks: [ smarthub ]
    restart: unless-stopped

  producer:
    build: ./producer
    container_name: producer
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - TOPIC=energy_readings
      - HOUSEHOLDS=50
      - TICK_SECONDS=15
      - SPEEDUP=30
      - JITTER_PCT=10
    depends_on:
      kafka:
        condition: service_healthy
    networks: [ smarthub ]
    restart: unless-stopped

  spark-submit:
    image: apache/spark:3.5.1
    container_name: spark-submit
    depends_on: [ spark-master, spark-worker, kafka ]
    environment:
      - RUN_MODE=demo
      - PYSPARK_PYTHON=python3
      - ALERTS_CONSOLE=1
      - CHECKPOINT_ROOT=/tmp/chk
    volumes:
      - ./spark:/app
    ports:
      - "4040:4040"
    networks: [ smarthub ]            # ← DODAJ TO
    command:
      - /bin/bash
      - -lc
      - >
        mkdir -p /tmp/.ivy2/cache /tmp/chk &&
        /opt/spark/bin/spark-submit
        --master spark://spark-master:7077
        --conf spark.driver.extraJavaOptions="-Duser.home=/tmp -Divy.home=/tmp/.ivy2 -Divy.cache.dir=/tmp/.ivy2/cache"
        --conf spark.executor.extraJavaOptions="-Duser.home=/tmp -Divy.home=/tmp/.ivy2 -Divy.cache.dir=/tmp/.ivy2/cache"
        --conf spark.sql.session.timeZone=UTC
        --conf spark.sql.shuffle.partitions=4
        --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
        /app/stream.py
    restart: unless-stopped
  ui:
    build: ./ui
    container_name: ui
    environment:
      - BOOTSTRAP_SERVERS=kafka:9092
      - ALERTS_TOPIC=energy_alerts
      - MAX_ROWS=5000
    depends_on:
      kafka:
        condition: service_healthy
    networks: [ smarthub ]
    ports: ["8501:8501"]
    restart: unless-stopped

    

networks:
  smarthub: {}
